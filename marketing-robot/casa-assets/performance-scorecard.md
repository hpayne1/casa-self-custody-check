# Performance scorecard (mixed: trust + engagement + conversions)

Use this after publishing. It’s designed to optimize **quality over quantity** while still learning from performance.

## Weights (defaults)

- **Trust / credibility rubric**: 50%
- **Engagement**: 30%
- **Conversions**: 20%

Adjust weights only if you can justify why (e.g., “trust was stable; optimizing for CTR in a non-crisis campaign”).

## Trust / credibility rubric (1–5 each)

Score each dimension:

1. **Credibility / restraint**: avoids absolutes, avoids speculation, acknowledges unknowns.
2. **Helpfulness**: gives actionable next steps, not just commentary.
3. **Institutional tone**: composed, first-class financial institution posture.
4. **Clarity**: plain language, scannable, minimal jargon.
5. **Empathy**: no dunking/schadenfreude; acknowledges stress and losses.
6. **Product truth**: consistent with `product-facts.md` (no hallucinated features/pricing).

## Engagement (normalize to 1–5)

Pick what you have access to. Normalize within your own baseline over time:

- Impressions (optional)
- Likes
- Reposts
- Replies
- Bookmarks
- Link clicks / CTR (if available)

## Conversions (normalize to 1–5)

If you have attribution:

- Sign-ups
- Trial starts
- Demo requests
- App installs

## Global vs scenario-specific learnings

When you write learnings, split them:

- **global_CASA_quality**: applies to all runs (tone, structure, clarity, restraint).
- **scenario_specific**: only applies when `Scenario_tag` matches (e.g., `ExchangeHack`).

